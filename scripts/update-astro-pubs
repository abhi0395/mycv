#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import importlib.util
from operator import itemgetter
import re
import ads

def fix_latex_typesetting(sorted_papers):
    for papers in sorted_papers:
        title = papers["title"]
        title = (title
                 .replace('{\\&}lt;', '<')
                 .replace('{\\&}gt;', '>')
                 .replace('{\\&}amp;', '&'))
        papers["title"] = title
    return sorted_papers


def remove_arxiv_duplicates(sorted_papers):
    """
    This function removes duplicate dictionaries from a list based on the "title" key.
    If duplicates are found, it removes the one where the "doi" key contains "arXiv".
    """
    def normalize_title(title):
        # Convert to lowercase and strip extra spaces
        title = ' '.join(title.lower().split())

        # General normalization for chemical notations, removing spaces between elements and numbers
        title = re.sub(r'(?<=\w)\s+(?=\w)', '', title)

        return title

    title_dict = {}
    removed_duplicates = []

    for item in sorted_papers:
        title = item.get("title", "")
        normalized_title = normalize_title(title)
        doi = item.get("doi", "")

        if normalized_title in title_dict:
            # If "arXiv" is in the current item's doi, add it to removed duplicates
            if "arXiv" in doi:
                removed_duplicates.append(item)
                continue
            # If "arXiv" is in the existing item's doi, replace it and add the old item to removed duplicates
            elif "arXiv" in title_dict[normalized_title].get("doi", ""):
                removed_duplicates.append(title_dict[normalized_title])
                title_dict[normalized_title] = item
            else:
                removed_duplicates.append(item)
        else:
            title_dict[normalized_title] = item

    # Return the cleaned list and the list of removed duplicates
    cleaned_list = list(title_dict.values())
    for i, clean in enumerate(cleaned_list):
        if clean["doctype"] in ["inproceedings", "phdthesis"]:
           cleaned_list.pop(i)
    return cleaned_list


here = os.path.dirname(os.path.abspath(__file__))
spec = importlib.util.spec_from_file_location(
    "utf8totex", os.path.join(here, "utf8totex.py")
)
utf8totex = importlib.util.module_from_spec(spec)
spec.loader.exec_module(utf8totex)

def get_papers(author):
    papers = list(
        ads.SearchQuery(
            author=author,
            q='galaxy',
            fl=[
                "id",
                "title",
                "author",
                "doi",
                "year",
                "pubdate",
                "pub",
                "volume",
                "page",
                "identifier",
                "doctype",
                "citation_count",
                "bibcode",
            ],
            max_pages=100,
        )
    )
    dicts = []
    for paper in papers:
        if paper.identifier is None:
            continue
        aid = [
            ":".join(t.split(":")[1:])
            for t in paper.identifier
            if t.startswith("arXiv:")
        ]
        for t in paper.identifier:
            if len(t.split(".")) != 2:
                continue
            try:
                list(map(int, t.split(".")))
            except ValueError:
                pass
            else:
                aid.append(t)
        try:
            page = int(paper.page[0])
        except (ValueError, TypeError):
            page = None
            if paper.page is not None and paper.page[0].startswith("arXiv:"):
                aid.append(":".join(paper.page[0].split(":")[1:]))

        if "LiteBIRD" not in paper.title[0] and paper.doctype!="phdthesis":
            dicts.append(
                dict(
                    doctype=paper.doctype,
                    authors=list(map(utf8totex.utf8totex, paper.author)),
                    year=paper.year,
                    pubdate=paper.pubdate,
                    doi=paper.doi[0] if paper.doi is not None else None,
                    title=utf8totex.utf8totex(paper.title[0].replace(" &amp; ", " & ")),
                    pub=paper.pub,
                    volume=paper.volume,
                    page=page,
                    arxiv=aid[0] if len(aid) else None,
                    citations=(
                        paper.citation_count if paper.citation_count is not None else 0
                    ),
                    url="https://ui.adsabs.harvard.edu/abs/" + paper.bibcode,
                )
            )
        else:
            continue
    sorted_papers = sorted(dicts, key=itemgetter("pubdate"), reverse=True)
    sorted_papers = remove_arxiv_duplicates(sorted_papers) # in case any arxiv duplicates or inproceedings are found...
    sorted_papers = fix_latex_typesetting(sorted_papers)
    return sorted_papers


if __name__ == "__main__":
    papers = get_papers("Anand, Abhijeet")
    with open("data/pubs.json", "w") as f:
        json.dump(papers, f, sort_keys=True, indent=2, separators=(",", ": "))
